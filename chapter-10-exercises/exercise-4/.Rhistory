getwd()
# Use the `read.csv()` functoin to read the data from the `data/gates_money.csv`
# file into a variable called `grants` using the `read.csv()`
# Be sure to set your working directory in RStudio, and do NOT treat strings as
# factors!
setwd(".'")
# Use the `read.csv()` functoin to read the data from the `data/gates_money.csv`
# file into a variable called `grants` using the `read.csv()`
# Be sure to set your working directory in RStudio, and do NOT treat strings as
# factors!
setwd(".")
getwd()
# part 3.2
setwd(".")
getwd()
# part 3.2
setwd("~")
getwd()
# part 3.2
setwd("~/UW_WORK/Info201/a3-using-data-serrat839")
# part 3.2
setwd("~/UW_WORK/Info201/a3-using-data-serrat839/")
# part 3.2
setwd("~/UW_WORK/INFO_201/a3-using-data-serrat839/")
# Use the `read.csv()` functoin to read the data from the `data/gates_money.csv`
# file into a variable called `grants` using the `read.csv()`
# Be sure to set your working directory in RStudio, and do NOT treat strings as
# factors!
setwd("~/UW_WORK/INFO_201/book-exercises/chapter-10-exercises/exercise-4/")
grants <- read.csv("./data/gates_money.csv", stringsAsFactors = F)
# Create a variable `organization` that contains the `organization` column of
# the dataset
organization <- grants$organization
# Confirm that the "organization" column is a vector using the `is.vector()`
# function.
# This is a useful debugging tip if you hit errors later!
is.vector(organization)
# What was the mean grant value?
mean(grants$total_amount)
# What was the dollar amount of the largest grant?
max(grants$total_amount)
# What was the dollar amount of the smallest grant?
min(grants$total_amount)
# Which organization received the largest grant?
grants[grants$total_amount == max(grants$total_amount), "organization"]
# Which organization received the smallest grant?
grants[grants$total_amount == min(grants$total_amount), "organization"]
# Use the View function to look at the loaded data
View(grants)
# How many grants were awarded in 2010?
nrow(grants[grants$start_year == 2010, ])
# How many grants were awarded in 2010?
nrow(grants[grants$start_year == 2010 & grants$total_amount > 10000, ])
knitr::opts_chunk$set(echo = TRUE)
```{r tweet_setup, message=FALSE}
library(xml2)
library(rvest)
library(stringr)
library(qdap)
library(qdap)
library(dplyr)
library(tm)
library(wordcloud)
library(plotrix)
library(dendextend)
library(ggplot2)
library(ggthemes)
library(RWeka)
library(tidytext)
library(stringr)
library(stringr)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(RWeka)
library(tidytext)
library(stringr)
library(dplyr)
secret1_data <- read.delim("./keys.txt", header=FALSE, sep = "\n", stringsAsFactors = FALSE)
library(dplyr)
secret1_data <- read.delim("./keys.txt", header=FALSE, sep = "\n", stringsAsFactors = FALSE)
secret_data <- secret1_data$V1
secret_data <- secret1_data$V1
api_key <- secret_data[1]
api_secret <-secret_data[2]
token <-secret_data[3]
token_secret <-secret_data[4]
setup_twitter_oauth(api_key, api_secret, token, token_secret)
setup_twitter_oauth(api_key, api_secret, token, token_secret)
library(twitteR)
setup_twitter_oauth(api_key, api_secret, token, token_secret)
userScrape <- function(handle) {
# 3200 is the max tweets we can request from twitters api
if (substr(handle,1,1) != '@') {
handle <- paste('@', handle, sep = "")
}
tweets <- userTimeline(handle, n= 3200, includeRts = TRUE, excludeReplies = FALSE)
tweets <- twListToDF(tweets)
return(tweets)
}
userScrape <- function(handle) {
# 3200 is the max tweets we can request from twitters api
if (substr(handle,1,1) != '@') {
handle <- paste('@', handle, sep = "")
}
tweets <- userTimeline(handle, n= 3200, includeRts = TRUE, excludeReplies = FALSE)
tweets <- twListToDF(tweets)
return(tweets)
}
tweet_gettr <- function(handle, output) {
tweets <- userScrape(handle)
tweets <- tweets$text
tweets <- str_replace_all(tweets, "'", "")
tweets <- str_replace_all(tweets, "\n", "newline")
fileConn <- file(output)
writeLines(tweets, fileConn, sep = "\n")
close(fileConn)
return(tweets)
}
read_txt <- function(file){
return(scan(file, character(), quote="", sep = "\n"))
}
# ideally this would be an object we can query over and over
make_sentence <- function(corpus, prompt = "") {
# step one
corpus <- paste(corpus, "THISISTHEENDOFTHESENTENCE") # vector of sentences
token_list <- strsplit(corpus, " ") # list of vectors of tokens
# No prompt available
if (nchar(prompt) == 0) {
# id first words and pick one
firsts <- c()
for (sentence in token_list) {
# pick the first token for each entry in token_list
firsts <- append(firsts, sentence[1])
}
# randomly select one of our first tokens, saving the word and also adding it to our sentence
selected_word <- sample(firsts, 1)
sentence <- selected_word
}
# User prompted the sentence maker
else {
# determine if the prompt is IN our corpus
regex_prompt <- paste('\\b', prompt, '\\b', sep = "")
prompt_is_valid <- F
for (sentence in token_list) {
if(!any(prompt_is_valid)) {
prompt_is_valid <- grepl(regex_prompt, sentence, ignore.case = T)
} else {
break
}
}
if (!any(prompt_is_valid)) {
return("This prompt is bad!")
}
# first word is prompt (or prompts last word)
selected_word <- prompt
sentence <- prompt
}
# convert our list of vectors into a giant vector of tokens
token_list <- unlist(token_list)
# add a period to the end of our vector just for saftey
token_list <- append(token_list, ".")
# make our giant vector lowercase to avoid case sensitivity
lowercase_token_list <- str_to_lower(token_list)
words <- 0
while(!grepl('[!?]|THISISTHEENDOFTHESENTENCE', sentence)) {
matches <- append(c(F), str_to_lower(selected_word) == lowercase_token_list)
after_match <- token_list[matches]
selected_word <- sample(after_match, 1)
sentence <- paste(sentence, selected_word)
words <- words + 1
}
sentence <- str_replace(sentence, " THISISTHEENDOFTHESENTENCE", "")
return(sentence)
}
tweet_corpus <- tweet_gettr("@BryanDMartin_", "../corporea/bryan.txt")
# add sentence param
# consider option to remove @'s (at least with first word)
#sapply(token_list, function(x) head(x,1))
#
make_sentence(tweet_corpus)
# add sentence param
# consider option to remove @'s (at least with first word)
#sapply(token_list, function(x) head(x,1))
#
make_sentence(tweet_corpus)
# add sentence param
# consider option to remove @'s (at least with first word)
#sapply(token_list, function(x) head(x,1))
#
make_sentence(tweet_corpus)
# add sentence param
# consider option to remove @'s (at least with first word)
#sapply(token_list, function(x) head(x,1))
#
make_sentence(tweet_corpus)
# to read in tweet corpus:
tweet_corpus2 <- read_txt("../corporea/sans.txt")
# add sentence param
# consider option to remove @'s (at least with first word)
#sapply(token_list, function(x) head(x,1))
#
make_sentence(tweet_corpus2)
# add sentence param
# consider option to remove @'s (at least with first word)
#sapply(token_list, function(x) head(x,1))
#
make_sentence(tweet_corpus2)
# add sentence param
# consider option to remove @'s (at least with first word)
#sapply(token_list, function(x) head(x,1))
#
make_sentence(tweet_corpus2)
knitr::opts_chunk$set(echo = TRUE)
# libs
library(xml2)
library(rvest)
library(stringr)
library(twitteR)
library(qdap)
library(dplyr)
library(tm)
library(wordcloud)
library(plotrix)
library(dendextend)
library(ggplot2)
library(ggthemes)
library(RWeka)
library(tidytext)
library(stringr)
library(dplyr)
# define a function to clean our text
# knowing my gc, i only want to make messages lowercase
clean_text <- function(x) {
# x <- ?replace_abbreviation(x)
# x <- replace_contraction(x)
# x <- replace_number(x)
# x <- replace_ordinal(x)
# x <- replace_symbol(x)
x <- tolower(x)
return(x)
}
cleaner_text <- function(x) {
x <- replace_abbreviation(x, abbreviation = read.csv("./abvs.csv"))
x <- replace_contraction(x)
x <- replace_number(x)
x <- replace_ordinal(x)
x <- replace_symbol(x)
x <- tolower(x)
return(x)
}
# for markov chain purposes
message_df$clean_content <- clean_text(message_df$content)
library(rjson)
rm(list=ls())
sender <- c()
timestamp <- c()
content <- c()
type <- c()
json_path <- "~/fb data/messages/inbox/NepalBOATProgram_JkQ3KCxoEg/message_1.json"
json_path2 <- "~/fb data/messages/inbox/Garrettchat_J9F_zmtbeA/message_1.json"
raw_info <- fromJSON(file=json_path2)
# get member info
members <- raw_info$participants
members <- unlist(members)
# empty placeholder lists and message jsons
messages <- raw_info$messages
# parse our json
for (message in messages) {
sender <- c(sender, message$sender_name)
timestamp <- c(timestamp, message$timestamp_ms)
if (is.null(message$content)){
content <- c(content, "photo")
} else {
content <- c(content, message$content)
}
type <- c(type, message$type)
}
# remove big data we dont need
rm(raw_info, messages)
message_df <- data.frame(sender, timestamp, content, type, stringsAsFactors = F)
# mark end of boat chat
end_of_boat <- 1574282872909
message_df$boat_chat <- message_df$timestamp >= end_of_boat
write.csv(message_df, file = "./corporea/fb_gc.csv")
content <- iconv(content, from="UTF-8", to="latin1")
content <- iconv(content, from="latin1", to="UTF-8")
# change emojis so they work
json_path <- "../fb data/messages/inbox/NepalBOATProgram_JkQ3KCxoEg/message_1.json"
json_path2 <- "../fb data/messages/inbox/Garrettchat_J9F_zmtbeA/message_1.json"
raw_info <- fromJSON(file=json_path2)
# get member info
members <- raw_info$participants
raw_info <- fromJSON(file=json_path2)
getwd()
install.packages("devtools")
library(devtools)
devtools::install_github("klutometis/roxygen")
devtools::install_github("klutometis/roxygen")
devtools::install_github("klutometis/roxygen")
devtools::install_github("klutometis/roxygen")
devtools::install_github("klutometis/roxygen")
uninstall("prettyunits")
devtools::install_github("klutometis/roxygen")
