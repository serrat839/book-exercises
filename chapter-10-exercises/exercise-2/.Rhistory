max(salaries$salary_ajustments)
# What was the "name" of the employee who received the highest raise?
salaries[salaries$salary_ajustments == max(salaries$salary_ajustments), "employees"]
# What was the largest decrease in salaries between the two years?
min(salaries$salary_ajustments)
# What was the name of the employee who recieved largest decrease in salary?
salaries[salaries$salary_ajustments == min(salaries$salary_ajustments), "employees"]
# What was the average salary change?
mean(salaries$salary_ajustments)
# For people who did not get a raise, how much money did they lose on average?
mean(salaries[salaries$salary_ajustments < 0, "salary_ajustments"])
## Consider: do the above averages match what you expected them to be based on
## how you generated the salaries?
# Write a .csv file of your salary data to your working directory
write.csv(salaries, "./salaries.txt")
# Create a data frame `salaries` by combining the 3 vectors you just made
# Remember to set `stringsAsFactors=FALSE`!
salaries <- data.frame(employees, salaries_2017, salary_ajustments, stringsAsFactors = FALSE)
# What was the name of the employee who recieved largest decrease in salary?
salaries[salaries$salary_ajustments == min(salaries$salary_ajustments), "employees"]
# What was the average salary change?
mean(salaries$salary_ajustments)
# For people who did not get a raise, how much money did they lose on average?
mean(salaries[salaries$salary_ajustments < 0, "salary_ajustments"])
# Write a .csv file of your salary data to your working directory
write.csv(salaries, "./salaries.txt")
# What was the 2018 salary of Employee 57
salaries[salaries$employees == "Employee 57", "salaries_2018"]
# How many employees got a raise?
sum(salaries$raise)
# What was the dollar value of the highest raise?
max(salaries$salary_ajustments)
# What was the "name" of the employee who received the highest raise?
salaries[salaries$salary_ajustments == max(salaries$salary_ajustments), "employees"]
# What was the largest decrease in salaries between the two years?
min(salaries$salary_ajustments)
# What was the name of the employee who recieved largest decrease in salary?
salaries[salaries$salary_ajustments == min(salaries$salary_ajustments), "employees"]
# What was the average salary change?
mean(salaries$salary_ajustments)
# For people who did not get a raise, how much money did they lose on average?
mean(salaries[salaries$salary_ajustments < 0, "salary_ajustments"])
# Write a .csv file of your salary data to your working directory
write.csv(salaries, "./salaries.txt")
?data.frame
# How many employees got a raise?
sum(salaries$raise)
# Exercise 2: working with data frames
# Create a vector of 100 employees ("Employee 1", "Employee 2", ... "Employee 100")
# Hint: use the `paste()` function and vector recycling to add a number to the word
# "Employee"
employees <- paste("Employee", 1:100)
# Create a vector of 100 random salaries for the year 2017
# Use the `runif()` function to pick random numbers between 40000 and 50000
salaries_2017 <- runif(100, 40000, 50000)
# Create a vector of 100 annual salary adjustments between -5000 and 10000.
# (A negative number represents a salary decrease due to corporate greed)
# Again use the `runif()` function to pick 100 random numbers in that range.
salary_ajustments <- runif(100, -5000, 10000)
# Create a data frame `salaries` by combining the 3 vectors you just made
# Remember to set `stringsAsFactors=FALSE`!
salaries <- data.frame(employees, salaries_2017, salary_ajustments, stringsAsFactors = FALSE)
View(salaries)
# Add a column to the `salaries` data frame that represents each person's
# salary in 2018 (e.g., with the salary adjustment added in).
salaries$salaries_2018 <- salaries$salaries_2017 +salaries$salary_ajustments
# Add a column to the `salaries` data frame that has a value of `TRUE` if the
# person got a raise (their salary went up)
salaries$raise <- salaries$salaries_2018 > salaries$salaries_2017
### Retrieve values from your data frame to answer the following questions
### Note that you should get the value as specific as possible (e.g., a single
### cell rather than the whole row!)
# What was the 2018 salary of Employee 57
salaries[salaries$employees == "Employee 57", "salaries_2018"]
# How many employees got a raise?
sum(salaries$raise)
# What was the dollar value of the highest raise?
max(salaries$salary_ajustments)
# What was the "name" of the employee who received the highest raise?
salaries[salaries$salary_ajustments == max(salaries$salary_ajustments), "employees"]
# What was the largest decrease in salaries between the two years?
min(salaries$salary_ajustments)
# What was the name of the employee who recieved largest decrease in salary?
salaries[salaries$salary_ajustments == min(salaries$salary_ajustments), "employees"]
# What was the average salary change?
mean(salaries$salary_ajustments)
# For people who did not get a raise, how much money did they lose on average?
mean(salaries[salaries$salary_ajustments < 0, "salary_ajustments"])
## Consider: do the above averages match what you expected them to be based on
## how you generated the salaries?
# Write a .csv file of your salary data to your working directory
write.csv(salaries, "./salaries.txt")
?data.frame
# How many employees got a raise?
sum(salaries$raise)
knitr::opts_chunk$set(echo = TRUE)
library(xml2)
library(rvest)
library(stringr)
library(twitteR)
library(qdap)
library(dplyr)
library(tm)
library(wordcloud)
library(plotrix)
library(dendextend)
library(ggplot2)
library(ggthemes)
library(RWeka)
library(tidytext)
library(stringr)
library(dplyr)
secret1_data <- read.delim("keys.txt", header=FALSE, sep = "\n", stringsAsFactors = FALSE)
secret1_data <- read.delim("./keys.txt", header=FALSE, sep = "\n", stringsAsFactors = FALSE)
secret_data <- secret1_data$V1
api_key <- secret_data[1]
api_secret <-secret_data[2]
token <-secret_data[3]
token_secret <-secret_data[4]
setup_twitter_oauth(api_key, api_secret, token, token_secret)
userScrape <- function(handle) {
# 3200 is the max tweets we can request from twitters api
if (substr(handle,1,1) != '@') {
handle <- paste('@', handle, sep = "")
}
tweets <- userTimeline(handle, n= 3200, includeRts = TRUE, excludeReplies = FALSE)
tweets <- twListToDF(tweets)
return(tweets)
}
tweet_gettr <- function(handle, output) {
tweets <- userScrape(handle)
tweets <- tweets$text
tweets <- str_replace_all(tweets, "'", "")
tweets <- str_replace_all(tweets, "\n", "newline")
fileConn <- file(output)
writeLines(tweets, fileConn, sep = "\n")
close(fileConn)
return(tweets)
}
read_txt <- function(file){
return(scan(file, character(), quote="", sep = "\n"))
}
# ideally this would be an object we can query over and over
make_sentence <- function(corpus, prompt = "") {
# step one
corpus <- paste(corpus, "THISISTHEENDOFTHESENTENCE") # vector of sentences
token_list <- strsplit(corpus, " ") # list of vectors of tokens
# No prompt available
if (nchar(prompt) == 0) {
# id first words and pick one
firsts <- c()
for (sentence in token_list) {
# pick the first token for each entry in token_list
firsts <- append(firsts, sentence[1])
}
# randomly select one of our first tokens, saving the word and also adding it to our sentence
selected_word <- sample(firsts, 1)
sentence <- selected_word
}
# User prompted the sentence maker
else {
# determine if the prompt is IN our corpus
regex_prompt <- paste('\b', prompt, '\b', sep = "")
prompt_is_valid <- F
for (sentence in token_list) {
if(!any(prompt_is_valid)) {
prompt_is_valid <- grepl(regex_prompt, sentence, ignore.case = T)
print(prompt_is_valid)
} else {
break
}
}
if (!any(prompt_is_valid)) {
return("This prompt is bad!")
}
# first word is prompt (or prompts last word)
selected_word <- prompt
sentence <- prompt
}
# convert our list of vectors into a giant vector of tokens
token_list <- unlist(token_list)
# add a period to the end of our vector just for saftey
token_list <- append(token_list, ".")
# make our giant vector lowercase to avoid case sensitivity
lowercase_token_list <- str_to_lower(token_list)
words <- 0
while(!grepl('[.!?]|THISISTHEENDOFTHESENTENCE', sentence)) {
matches <- append(c(F), str_to_lower(selected_word) == lowercase_token_list)
after_match <- token_list[matches]
selected_word <- sample(after_match, 1)
sentence <- paste(sentence, selected_word)
words <- words + 1
}
sentence <- str_replace(sentence, " THISISTHEENDOFTHESENTENCE", "")
return(sentence)
}
make_sentence(tweet_corpus)
# to read in tweet corpus:
tweet_corpus2 <- read_txt("./corporea/trump.txt")
# to read in tweet corpus:
tweet_corpus2 <- read_txt("../corporea/trump.txt")
make_sentence(tweet_corpus)
make_sentence(tweet_corpus2)
make_sentence(tweet_corpus2)
prompt_corpus <- c("I like to eat apples and bananas")
make_sentence(prompt_corpus, "a")
make_sentence(prompt_corpus, "eat")
prompt_corpus <- c("I like to eat apples and bananas")
make_sentence(prompt_corpus, "eat")
make_sentence(prompt_corpus, "and")
debug(make_sentence)
make_sentence(prompt_corpus, "and")
corpus
token_list
regex_prompt
token_list
prompt_is_valid
regex)prompt
regex_prompt
regex_prompt = "and"
grepl(regex_prompt, sentence, ignore.case = T)
sentence
regex_prompt <- paste('\b', prompt, '\b', sep = "")
?grepl
grepl(regex_prompt, sentence, ignore.case = T)
regex_prompt
sentence
gregexpr(regex_prompt, sentence, ignore.case = T)
grepl(regex_prompt, sentence, ignore.case = T)
grepl(pattern = regex_prompt, sentence, ignore.case = T)
regex_prompt <- paste('\\b', prompt, '\\b', sep = "")
prompt_is_valid <- F
for (sentence in token_list) {
if(!any(prompt_is_valid)) {
prompt_is_valid <- grepl(regex_prompt, sentence, ignore.case = T)
print(prompt_is_valid)
} else {
break
}
}
regex_prompt
prompt_is_valid
grepl(regex_prompt, sentence, ignore.case = T)
# ideally this would be an object we can query over and over
make_sentence <- function(corpus, prompt = "") {
# step one
corpus <- paste(corpus, "THISISTHEENDOFTHESENTENCE") # vector of sentences
token_list <- strsplit(corpus, " ") # list of vectors of tokens
# No prompt available
if (nchar(prompt) == 0) {
# id first words and pick one
firsts <- c()
for (sentence in token_list) {
# pick the first token for each entry in token_list
firsts <- append(firsts, sentence[1])
}
# randomly select one of our first tokens, saving the word and also adding it to our sentence
selected_word <- sample(firsts, 1)
sentence <- selected_word
}
# User prompted the sentence maker
else {
# determine if the prompt is IN our corpus
regex_prompt <- paste('\\b', prompt, '\\b', sep = "")
prompt_is_valid <- F
for (sentence in token_list) {
if(!any(prompt_is_valid)) {
prompt_is_valid <- grepl(regex_prompt, sentence, ignore.case = T)
print(prompt_is_valid)
} else {
break
}
}
if (!any(prompt_is_valid)) {
return("This prompt is bad!")
}
# first word is prompt (or prompts last word)
selected_word <- prompt
sentence <- prompt
}
# convert our list of vectors into a giant vector of tokens
token_list <- unlist(token_list)
# add a period to the end of our vector just for saftey
token_list <- append(token_list, ".")
# make our giant vector lowercase to avoid case sensitivity
lowercase_token_list <- str_to_lower(token_list)
words <- 0
while(!grepl('[.!?]|THISISTHEENDOFTHESENTENCE', sentence)) {
matches <- append(c(F), str_to_lower(selected_word) == lowercase_token_list)
after_match <- token_list[matches]
selected_word <- sample(after_match, 1)
sentence <- paste(sentence, selected_word)
words <- words + 1
}
sentence <- str_replace(sentence, " THISISTHEENDOFTHESENTENCE", "")
return(sentence)
}
undebug(make_sentence)
# ideally this would be an object we can query over and over
make_sentence <- function(corpus, prompt = "") {
# step one
corpus <- paste(corpus, "THISISTHEENDOFTHESENTENCE") # vector of sentences
token_list <- strsplit(corpus, " ") # list of vectors of tokens
# No prompt available
if (nchar(prompt) == 0) {
# id first words and pick one
firsts <- c()
for (sentence in token_list) {
# pick the first token for each entry in token_list
firsts <- append(firsts, sentence[1])
}
# randomly select one of our first tokens, saving the word and also adding it to our sentence
selected_word <- sample(firsts, 1)
sentence <- selected_word
}
# User prompted the sentence maker
else {
# determine if the prompt is IN our corpus
regex_prompt <- paste('\\b', prompt, '\\b', sep = "")
prompt_is_valid <- F
for (sentence in token_list) {
if(!any(prompt_is_valid)) {
prompt_is_valid <- grepl(regex_prompt, sentence, ignore.case = T)
print(prompt_is_valid)
} else {
break
}
}
if (!any(prompt_is_valid)) {
return("This prompt is bad!")
}
# first word is prompt (or prompts last word)
selected_word <- prompt
sentence <- prompt
}
# convert our list of vectors into a giant vector of tokens
token_list <- unlist(token_list)
# add a period to the end of our vector just for saftey
token_list <- append(token_list, ".")
# make our giant vector lowercase to avoid case sensitivity
lowercase_token_list <- str_to_lower(token_list)
words <- 0
while(!grepl('[.!?]|THISISTHEENDOFTHESENTENCE', sentence)) {
matches <- append(c(F), str_to_lower(selected_word) == lowercase_token_list)
after_match <- token_list[matches]
selected_word <- sample(after_match, 1)
sentence <- paste(sentence, selected_word)
words <- words + 1
}
sentence <- str_replace(sentence, " THISISTHEENDOFTHESENTENCE", "")
return(sentence)
}
prompt_corpus <- c("I like to eat apples and bananas")
make_sentence(prompt_corpus, "and")
make_sentence(tweet_corpus2, "army")
# ideally this would be an object we can query over and over
make_sentence <- function(corpus, prompt = "") {
# step one
corpus <- paste(corpus, "THISISTHEENDOFTHESENTENCE") # vector of sentences
token_list <- strsplit(corpus, " ") # list of vectors of tokens
# No prompt available
if (nchar(prompt) == 0) {
# id first words and pick one
firsts <- c()
for (sentence in token_list) {
# pick the first token for each entry in token_list
firsts <- append(firsts, sentence[1])
}
# randomly select one of our first tokens, saving the word and also adding it to our sentence
selected_word <- sample(firsts, 1)
sentence <- selected_word
}
# User prompted the sentence maker
else {
# determine if the prompt is IN our corpus
regex_prompt <- paste('\\b', prompt, '\\b', sep = "")
prompt_is_valid <- F
for (sentence in token_list) {
if(!any(prompt_is_valid)) {
prompt_is_valid <- grepl(regex_prompt, sentence, ignore.case = T)
} else {
break
}
}
if (!any(prompt_is_valid)) {
return("This prompt is bad!")
}
# first word is prompt (or prompts last word)
selected_word <- prompt
sentence <- prompt
}
# convert our list of vectors into a giant vector of tokens
token_list <- unlist(token_list)
# add a period to the end of our vector just for saftey
token_list <- append(token_list, ".")
# make our giant vector lowercase to avoid case sensitivity
lowercase_token_list <- str_to_lower(token_list)
words <- 0
while(!grepl('[.!?]|THISISTHEENDOFTHESENTENCE', sentence)) {
matches <- append(c(F), str_to_lower(selected_word) == lowercase_token_list)
after_match <- token_list[matches]
selected_word <- sample(after_match, 1)
sentence <- paste(sentence, selected_word)
words <- words + 1
}
sentence <- str_replace(sentence, " THISISTHEENDOFTHESENTENCE", "")
return(sentence)
}
make_sentence(tweet_corpus2, "army")
make_sentence(tweet_corpus2, "army")
tweet_corpus <- tweet_gettr("@bryandmartin_", "./corporea/bryan.txt")
tweet_corpus <- tweet_gettr("@BryanDMartin_", "./corporea/bryan.txt")
tweet_corpus <- tweet_gettr("@BryanDMartin_", "../corporea/bryan.txt")
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
tweet_corpus <- tweet_gettr("@AmyDWillis", "../corporea/amy.txt")
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
# ideally this would be an object we can query over and over
make_sentence <- function(corpus, prompt = "") {
# step one
corpus <- paste(corpus, "THISISTHEENDOFTHESENTENCE") # vector of sentences
token_list <- strsplit(corpus, " ") # list of vectors of tokens
# No prompt available
if (nchar(prompt) == 0) {
# id first words and pick one
firsts <- c()
for (sentence in token_list) {
# pick the first token for each entry in token_list
firsts <- append(firsts, sentence[1])
}
# randomly select one of our first tokens, saving the word and also adding it to our sentence
selected_word <- sample(firsts, 1)
sentence <- selected_word
}
# User prompted the sentence maker
else {
# determine if the prompt is IN our corpus
regex_prompt <- paste('\\b', prompt, '\\b', sep = "")
prompt_is_valid <- F
for (sentence in token_list) {
if(!any(prompt_is_valid)) {
prompt_is_valid <- grepl(regex_prompt, sentence, ignore.case = T)
} else {
break
}
}
if (!any(prompt_is_valid)) {
return("This prompt is bad!")
}
# first word is prompt (or prompts last word)
selected_word <- prompt
sentence <- prompt
}
# convert our list of vectors into a giant vector of tokens
token_list <- unlist(token_list)
# add a period to the end of our vector just for saftey
token_list <- append(token_list, ".")
# make our giant vector lowercase to avoid case sensitivity
lowercase_token_list <- str_to_lower(token_list)
words <- 0
while(!grepl('[!?]|THISISTHEENDOFTHESENTENCE', sentence)) {
matches <- append(c(F), str_to_lower(selected_word) == lowercase_token_list)
after_match <- token_list[matches]
selected_word <- sample(after_match, 1)
sentence <- paste(sentence, selected_word)
words <- words + 1
}
sentence <- str_replace(sentence, " THISISTHEENDOFTHESENTENCE", "")
return(sentence)
}
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
make_sentence(tweet_corpus)
tweet_corpus <- tweet_gettr("@BryanDMartin_", "../corporea/bryan.txt")
debug(make_sentence)
# add sentence param
make_sentence(tweet_corpus)
firsts
head(token_list)
lapply(token_list, head(,1))
lapply(token_list, function(x) head(x,1))
sapply(token_list, function(x) head(x,1))
selected_word
sentence
token_list
matches
?append
undebug(make_sentence)
install.packages("profvis")
profvis::profvis({make_sentence(tweet_corpus)})
undebug(make_sentence)
make_sentence(tweet_corpus)
profvis::profvis({make_sentence(tweet_corpus)})
make_sentence(tweet_corpus)
